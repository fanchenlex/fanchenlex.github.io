<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhuo&#39;s Blog</title>
  
  
  <link href="https://fanchenlex.github.io/atom.xml" rel="self"/>
  
  <link href="https://fanchenlex.github.io/"/>
  <updated>2025-11-02T05:25:55.974Z</updated>
  <id>https://fanchenlex.github.io/</id>
  
  <author>
    <name>Wenzhuo Li</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RDP | RSS 2025 | Paper Reading</title>
    <link href="https://fanchenlex.github.io/reandings/RDP/"/>
    <id>https://fanchenlex.github.io/reandings/RDP/</id>
    <published>2025-11-02T05:35:50.000Z</published>
    <updated>2025-11-02T05:25:55.974Z</updated>
    
    <content type="html"><![CDATA[<p>Reactive Diffusion Policy:  Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation</p><h3 id="这篇文章通过软件与硬件两套设计实现了模型面对需要精细操作任务时的快速相应能力，其中的motivation很有意义。"><a href="#这篇文章通过软件与硬件两套设计实现了模型面对需要精细操作任务时的快速相应能力，其中的motivation很有意义。" class="headerlink" title="这篇文章通过软件与硬件两套设计实现了模型面对需要精细操作任务时的快速相应能力，其中的motivation很有意义。"></a>这篇文章通过软件与硬件两套设计实现了模型面对需要精细操作任务时的快速相应能力，其中的motivation很有意义。</h3><span id="more"></span><table style="width:100%; border-collapse:collapse;">  <thead style="border-bottom:1px solid #ccc;">    <tr>      <th style="padding:8px; text-align:center;">工作类型(首次/改进)</th>      <th style="padding:8px; text-align:center;">技术路线</th>      <th style="padding:8px; text-align:center;">创新点</th>      <th style="padding:8px; text-align:center;">日期</th>    </tr>  </thead>  <tbody>    <tr>      <td style="padding:8px; text-align:center;">首次</td>      <td style="padding:8px; text-align:center;">快慢系统</td>      <td style="padding:8px; text-align:center;">双系统+力触觉的新思想</td>      <td style="padding:8px; text-align:center;">2025-10-04</td>    </tr>  </tbody></table><h2 id="1-What？"><a href="#1-What？" class="headerlink" title="1.What？"></a>1.What？</h2><p>提出TactAR（一种通过增强现实提供实时触觉反馈的低成本遥操作系统）<br>Reactive Diffusion Policy（RDP，一种新型慢快视觉触觉模仿学习算法）</p><h2 id="2-Why？"><a href="#2-Why？" class="headerlink" title="2.Why？"></a>2.Why？</h2><p>神经科学领域的一些研究成果表明，人类在执行接触密集型任务时，其过程可分为两个组成部分：1）前馈&#x2F;预测性动作；2）基于感官反馈（如触觉信号）的闭环微调。受此启发，我们致力于开发一种能模拟人类在执行复杂接触密集型任务时控制模式的机器人学习系统。</p><h2 id="3-How？"><a href="#3-How？" class="headerlink" title="3.How？"></a>3.How？</h2><p><img src="/img/RDP.png" alt="RDP Overview"><br>RDP采用双层架构：<br>（1）在低频下预测潜在空间中高层动作分块的慢速潜在扩散策略；<br>（2）在高频下实现闭环触觉反馈控制的快速非对称分词器<br>TactAR：Meta Quest3提供实时触觉&#x2F;力反馈<br>慢速网络（潜在扩散策略LDP）作为神经规划器，以低频（1-2Hz）在潜在空间预测高层动作块，类比预测性动作；快速网络（非对称分词器AT）作为可学习阻抗控制器，基于高频触觉反馈（20-30Hz）微调潜在动作块，类比闭环微调。该分层结构使慢速网络通过扩散模型和动作分块保持对复杂非马尔可夫动作的建模能力，而快速网络借助实时触觉反馈实现精确力控与快速响应的闭环控制。<br>首先通过训练非对称分词器将原始动作块编码至潜空间，其解码器将瞬时触觉表征与潜动作块分离作为输入；<br>非对称的含义是仅在解码器中将触觉表示作为输入。这种刻意设计的结构不对称性是为了确保潜在动作片段仅保留高层反馈策略，而精确位置则由解码器借助触觉信息进行预测。<br>策略学习阶段，慢速潜扩散策略(LDP)以类似扩散策略的方式根据观测数据预测潜动作块；<br>下采样后的潜在表示降低了计算成本；<br>AT中的不对称设计可将具有挑战性的反应行为排除在潜在动作片段之外，从而降低潜在扩散策略在低频观测下的学习难度并增强其泛化能力<br>推理阶段低频采样潜动作块，在每个块内执行动作时，最新触觉表征实时输入AT解码器以预测下一帧实际动作<br>采用相对末端执行器轨迹进行动作表征：不直接计算连续帧间的增量动作（可能导致较大累积误差），而是通过计算相对于基准坐标系的相对变换，将绝对位姿轨迹转换为相对轨迹。<br>我们计算策略推理与动作执行产生的延迟，并舍弃模型预测的前几个动作步，以向机器人发送精确匹配的动作</p><h2 id="4-Takeaways"><a href="#4-Takeaways" class="headerlink" title="4.Takeaways:"></a>4.Takeaways:</h2><p>1.在复杂接触密集型任务中，单纯将触觉信号加入观测空间未必能提升性能<br>2.低维触觉嵌入在评估过程中对凝胶损伤或更换导致的纹理变化展现出更强鲁棒性<br>3.纯视觉输入的DP常预测出偏差轨迹导致较大接触力（如图8(b)失效案例2），需人工干预防止传感器损坏；而触觉嵌入的DP虽很少产生大接触力，却易在接触物体时陷入停滞（如图8(a)失效案例2及图8(c)失效案例）。这可能是因为其从数据中学到了反应行为（如接触时微抬、失联时下压），但由于开环执行动作块，缺乏精细调整能力，导致在不同接触状态间快速切换<br>4.RDP适用于不同触觉&#x2F;力传感器，RDP能同时使用左右夹爪的不同触觉传感器（左MCTac&#x2F;右GelSight）保持优异表现<br>5.我们推测力传感器可能因其相较于光学触觉传感器具有更低延迟和更小维度而呈现更优效果，这或许降低了网络的学习难度。RDP能即时响应外部干扰<br>6.在剥离任务中相对轨迹预测的表现远优于绝对动作预测，这可能是因为相对轨迹更易于轻量化快速策略模型的学习由此从触觉反馈中带来了一个更具泛化能力的反应策略。此外，相对轨迹还压缩了潜在空间，促进了潜在扩散策略的学习过程。我们还发现，延迟匹配通过确保动作块之间的平滑过渡并减少分布外（OOD）行为，对策略性能提升贡献显著。<br>7.数据质量对于模型能力提升很重要<br>局限性：<br>1.虽然TactAR能在AR中提供一定程度的触觉&#x2F;力反馈，但其直观性和效率仍不及人手直接操作<br>2.TactAR系统专为二指夹爪设计<br>3.RDP算法中的快速策略目前仅能响应高频触觉&#x2F;力输入信号，尚无法快速处理高频图像输入<br>4.RDP算法目前仅适用于单任务场景<br>未来工作：<br>1.进一步降低传感器与系统延迟来提升远程操作效率<br>2.将该系统及RDP算法拓展至配备触觉传感器的灵巧手<br>3.考虑将高频视觉输入融入快速网络<br>4.可通过采用类似RDP的非对称分词器替换VLA分词器，将RDP与视觉-语言-动作模型结合，从而在VLA中引入具有实时触觉&#x2F;力反馈的闭环控制反应行为。</p><p>最后放上因为研一人工智能课读论文环节的PPT(我正好选的是这一篇)：</p><iframe src="/pdf/RDP.pdf" width="100%" height="600px"></iframe>]]></content>
    
    
    <summary type="html">&lt;p&gt;Reactive Diffusion Policy:  Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation&lt;/p&gt;
&lt;h3 id=&quot;这篇文章通过软件与硬件两套设计实现了模型面对需要精细操作任务时的快速相应能力，其中的motivation很有意义。&quot;&gt;&lt;a href=&quot;#这篇文章通过软件与硬件两套设计实现了模型面对需要精细操作任务时的快速相应能力，其中的motivation很有意义。&quot; class=&quot;headerlink&quot; title=&quot;这篇文章通过软件与硬件两套设计实现了模型面对需要精细操作任务时的快速相应能力，其中的motivation很有意义。&quot;&gt;&lt;/a&gt;这篇文章通过软件与硬件两套设计实现了模型面对需要精细操作任务时的快速相应能力，其中的motivation很有意义。&lt;/h3&gt;</summary>
    
    
    
    <category term="readings" scheme="https://fanchenlex.github.io/categories/readings/"/>
    
    
    <category term="paper reading" scheme="https://fanchenlex.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>VLA-Adapter | arxiv 2025.9.11 | Paper Reading</title>
    <link href="https://fanchenlex.github.io/reandings/VLA-ADAPTER/"/>
    <id>https://fanchenlex.github.io/reandings/VLA-ADAPTER/</id>
    <published>2025-11-02T05:35:50.000Z</published>
    <updated>2025-11-02T05:39:18.973Z</updated>
    
    <content type="html"><![CDATA[<p>VLA-ADAPTER: AN EFFECTIVE PARADIGM FOR TINY-SCALE VISION-LANGUAGE-ACTION MODEL</p><h3 id="这篇文章通过设计了一个bridge-Attenion-block以及广泛的实验尝试验证了小模型也能够有很好的性能。"><a href="#这篇文章通过设计了一个bridge-Attenion-block以及广泛的实验尝试验证了小模型也能够有很好的性能。" class="headerlink" title="这篇文章通过设计了一个bridge Attenion block以及广泛的实验尝试验证了小模型也能够有很好的性能。"></a>这篇文章通过设计了一个bridge Attenion block以及广泛的实验尝试验证了小模型也能够有很好的性能。</h3><span id="more"></span><table style="width:100%; border-collapse:collapse;">  <thead style="border-bottom:1px solid #ccc;">    <tr>      <th style="padding:8px; text-align:center;">工作类型(首次/改进)</th>      <th style="padding:8px; text-align:center;">技术路线</th>      <th style="padding:8px; text-align:center;">创新点</th>      <th style="padding:8px; text-align:center;">日期</th>    </tr>  </thead>  <tbody>    <tr>      <td style="padding:8px; text-align:center;">首次</td>      <td style="padding:8px; text-align:center;">adapter</td>      <td style="padding:8px; text-align:center;">bridge Attention</td>      <td style="padding:8px; text-align:center;">2025-10-07</td>    </tr>  </tbody></table><h2 id="1-What？"><a href="#1-What？" class="headerlink" title="1.What？"></a>1.What？</h2><p>为了解决如何有效衔接视觉-语言表征与动作空间这一问题，提出VLA-Adapter这一创新范式，旨在降低VLA模型对大规模VLM和大量预训练的依赖。<br>首先系统分析了不同VL条件的有效性，并揭示了哪些条件对衔接感知与动作空间至关重要的关键发现。基于这些洞见，我们设计出带有桥接注意力机制的轻量化策略模块，可自主将最优条件注入动作空间。VLA-Adapter将充足的多模态信息传递至所提策略网络以生成动作，有效弥合了从视觉语言到动作的模态鸿沟。<br>该方法仅需0.5B参数的主干网络即可实现高性能，且无需任何机器人数据预训练。在仿真与真实机器人基准测试上的大量实验表明，VLA-Adapter不仅达到最先进性能水平，还实现了目前报道中最快的推理速度。得益于提出的先进桥接范式，VLA-Adapter仅需在单张消费级GPU上训练8小时即可构建强大VLA模型，大幅降低了VLA模型的部署门槛。</p><h2 id="2-Why？"><a href="#2-Why？" class="headerlink" title="2.Why？"></a>2.Why？</h2><p>（i）SFT扩展所需的大规模人工示教轨迹数据稀缺且成本高昂；<br>（ii）对存在分布偏移任务的泛化能力有限。<br>面对高维控制环境时，VLA模型仍存在多重瓶颈：依赖大规模视觉语言模型、微调速度缓慢、GPU显存消耗高以及推理效率（吞吐量）低下<br>大规模具身数据集进行预训练将VLM与专用策略网络相结合，使系统能够以端到端方式解析或生成多样化任务的动作序列。此外，双系统VLA架构近期受到关注，这类方法通常引入中间潜在标记连接VLM与策略网络，通过异步机制增强双系统协同，从而缓解动作生成过程中的延迟问题。因此，如何高效衔接视觉语言感知空间与动作空间已成为VLA模型设计的核心挑战。</p><h2 id="3-How？"><a href="#3-How？" class="headerlink" title="3.How？"></a>3.How？</h2><p><img src="/img/VLtoAparadigms.png" alt="Existing representative bridge paradigms from VL to A"><br>该视觉语言模型遵循PrismaticVLMs架构，包含M个网络层。在时间步t时，输入VLM的数据包含 ${\mathcal{X}_t^v,\mathcal{X}_t^g,\mathcal{L}_t,\mathcal{A}\mathcal{Q}_t}:$第三视角图像$\mathcal{X}_t^v$、夹爪图像$\mathcal{X}_t^g$、指令文本Lt以及动作查询$\mathcal{A}\mathcal{Q}_t$。输入$\mathcal{X}_t^v$和$\mathcal{X}_t^g$后，分别通过DINOv2和SigLIP提取视觉嵌入表示(每一个都是过两个编码器与openvla-oft输入方式一样)，Lt则进行分词处理。输出为指定层的原始潜在表示$\mathcal{C}_t^{\mathcal{R}}$和动作查询潜在表示 $\mathcal{C}^{\mathcal{A}\mathcal{Q}}_t$，二者共同作为策略网络的输入条件。输出是：xxxxxx<br>在第t个时间步，策略网络的输入包括：${\mathcal{C}_t^{\mathcal{R}},\mathcal{C}_t^{\mathcal{AQ}},\mathcal{A}_t^{\tau&#x3D;0},\mathcal{P}_t}$。τ表示策略网络的层级，满足τ ∈ Z+且0 ≤ τ ≤ M−1。At0是H步零初始化动作序列，经过层归一化（LN）和多层感知机（MLP）处理得到Ae t0 &#x3D; aet0, aet0+1, …, ae0 t+H−1。Pt为本体感知状态，通过两层MLP映射获得本体嵌入表征σ0(Pt)。网络输出为H步动作块AM−1 t。每层结构由桥式注意力模块和前馈网络（FFN）构成，桥式注意力架构如图5所示。<br><img src="/img/VLAAdapterframework.png" alt="The proposed VLA framework"><br><img src="/img/VLA-AdapterBridgeAttention.png" alt="The Policy with Bridge Attention"><br>桥注意力机制。提出的桥注意力旨在通过条件$\mathcal{C}_t^{\mathcal{R}}$和$\mathcal{C}_t^{\mathcal{AQ}}$最大限度地引导动作生成。每个桥注意力包含两个交叉注意力层和一个自注意力层。首个交叉注意力层中，$\mathcal{C}_t^{\mathcal{R}}$经MLP网络σ1处理得到K1、V1，动作潜在表示Ae_tτ作为Q1执行注意力计算，获得CA1(Ae_tτ, σ1($\mathcal{C}_t^{\mathcal{R}}$))。次个交叉注意力层需将$\mathcal{C}_t^{\mathcal{AQ}}$与σ0(Pt)拼接后经MLP网络σ2处理得到K2、V2，以Ae_tτ作为Q2获取CA2(Ae_tτ, σ2($\mathcal{C}_t^{\mathcal{AQ}}$, σ0(Pt)))。自注意力层中以Ae_tτ同时作为Q、K、V，执行SA(Ae_tτ, Ae_tτ)运算。为选择性地将特定$\mathcal{C}_t^{\mathcal{R}}$注入策略的动作空间，我们引入学习参数比率g来调节CA1(Ae_tτ, σ1($\mathcal{C}_t^{\mathcal{R}}$))的影响程度。g初始化为0值，并采用tanh激活函数使tanh(g)∈[-1,1]，以防止极端值破坏分布稳定性（Zhang等，2023）。最终将三个注意力输出拼接得到Ab_tτ</p><h2 id="4-Takeaways"><a href="#4-Takeaways" class="headerlink" title="4.Takeaways:"></a>4.Takeaways:</h2><p>关键发现1：在$\mathcal{C}_t^{\mathcal{R}}$方面，中层潜表征性能优于深层潜表征。深层$\mathcal{C}_t^{\mathcal{R}}$偏向语义信息而弱于动作生成。中层$\mathcal{C}_t^{\mathcal{R}}$能有效融合图像与文本信息，保留更丰富的多模态细节，有利于动作生成。<br>关键发现2：对于$\mathcal{C}^{\mathcal{A}\mathcal{Q}}_t$，深层潜表征表现优于其他层级。由于ActionQuery是从头开始训练的，深层$\mathcal{C}^{\mathcal{A}\mathcal{Q}}_t$聚合了更丰富的多模态细节，比浅层更能有效促进动作生成。<br>关键发现3：多层特征组合性能更优。我们观察到使用全层级特征通常优于单层特征，不仅提升性能，还能节省设计过程中的最佳层级选择时间。这种设计更具普适性。<br>条件判定。VLA-Adapter是否完全依赖$\mathcal{C}_t^{\mathcal{AQ}}$作为条件？答案是否定的。虽然全层$\mathcal{C}^{\mathcal{A}\mathcal{Q}}_t$优于$\mathcal{C}_t^{\mathcal{R}}$，但中间层$\mathcal{C}_t^{\mathcal{R}}$在某些复杂任务中表现更佳。对比结果如表1所示。因此，我们旨在通过利用$\mathcal{C}_t^{\mathcal{R}}$的特定知识来提升性能。<br>结论一：当视觉语言模型未经过机器人预训练时，VLA-Adapter的改进效果显著。<br>结论二：即使主干网络参数冻结，VLA-Adapter仍能保持强劲性能。<br>VLA-Adapter无需机器人预训练即可实现视觉语言模型的高效微调，其性能超越采用微型主干网络的基线方法<br>局限性&amp;未来工作：<br>1.经过大量具身数据预训练且模型规模微小，其在现实系统中的泛化能力有待提升<br>2.策略网络生成动作的质量取决于视觉语言模型提供的条件及其利用方式-&gt;深入探索这些条件以改进其表征能力并确保高效利用<br>3.VLA-Adapter的基础训练流程仍相对简单-&gt;可探索强化学习等复杂训练流程</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;VLA-ADAPTER: AN EFFECTIVE PARADIGM FOR TINY-SCALE VISION-LANGUAGE-ACTION MODEL&lt;/p&gt;
&lt;h3 id=&quot;这篇文章通过设计了一个bridge-Attenion-block以及广泛的实验尝试验证了小模型也能够有很好的性能。&quot;&gt;&lt;a href=&quot;#这篇文章通过设计了一个bridge-Attenion-block以及广泛的实验尝试验证了小模型也能够有很好的性能。&quot; class=&quot;headerlink&quot; title=&quot;这篇文章通过设计了一个bridge Attenion block以及广泛的实验尝试验证了小模型也能够有很好的性能。&quot;&gt;&lt;/a&gt;这篇文章通过设计了一个bridge Attenion block以及广泛的实验尝试验证了小模型也能够有很好的性能。&lt;/h3&gt;</summary>
    
    
    
    <category term="readings" scheme="https://fanchenlex.github.io/categories/readings/"/>
    
    
    <category term="paper reading" scheme="https://fanchenlex.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>RoboTwin 2.0 | arxiv 2025.8.27(preprint) | Paper Reading</title>
    <link href="https://fanchenlex.github.io/reandings/RoboTwin2.0/"/>
    <id>https://fanchenlex.github.io/reandings/RoboTwin2.0/</id>
    <published>2025-11-02T05:05:50.000Z</published>
    <updated>2025-11-02T03:50:46.249Z</updated>
    
    <content type="html"><![CDATA[<p>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</p><h3 id="这篇文章将Diffusion应用到3D空间。是在diffusion-policy基础上进一步提升了策略的能力。"><a href="#这篇文章将Diffusion应用到3D空间。是在diffusion-policy基础上进一步提升了策略的能力。" class="headerlink" title="这篇文章将Diffusion应用到3D空间。是在diffusion policy基础上进一步提升了策略的能力。"></a>这篇文章将Diffusion应用到3D空间。是在diffusion policy基础上进一步提升了策略的能力。</h3><span id="more"></span><table style="width:100%; border-collapse:collapse;">  <thead style="border-bottom:1px solid #ccc;">    <tr>      <th style="padding:8px; text-align:center;">工作类型(首次/改进)</th>      <th style="padding:8px; text-align:center;">技术路线</th>      <th style="padding:8px; text-align:center;">创新点</th>      <th style="padding:8px; text-align:center;">日期</th>    </tr>  </thead>  <tbody>    <tr>      <td style="padding:8px; text-align:center;">改进</td>      <td style="padding:8px; text-align:center;">simulation data collection</td>      <td style="padding:8px; text-align:center;">自动化仿真数据生成pipline的改进引入多模态反馈代码生成</td>      <td style="padding:8px; text-align:center;">2025-10-03</td>    </tr>  </tbody></table><h2 id="1-What？"><a href="#1-What？" class="headerlink" title="1.What？"></a>1.What？</h2><p>一个高效自动化采集无穷的高质量数据的框架</p><h2 id="2-Why？"><a href="#2-Why？" class="headerlink" title="2.Why？"></a>2.Why？</h2><p>缺少高质量训练数据；采集强度大难以规模化；仿真做自动化数据生成较困难；数据不够多样</p><h2 id="3-How？"><a href="#3-How？" class="headerlink" title="3.How？"></a>3.How？</h2><p><img src="/img/expertcodegen.png" alt="Expert code generation pipeline"><br>在1.0基础上增加了：<br>升级了API库，对代码进行了优化，提升了代码的可读性<br>覆盖了更多的技能<br>LLM生成部分增加了视觉反馈<br>在原来的点轴系统上设计了不同的语义轴支持抓取时使用不同的姿势<br>场景由单一白色变为不同的背景材质<br>增加了光照桌子高度的变化<br>轨迹上多语言指令<br>背景增加了其他物体</p><h2 id="4-Takeaways"><a href="#4-Takeaways" class="headerlink" title="4.Takeaways:"></a>4.Takeaways:</h2><p>1.在低自由度的机械臂上有明显涨点，在高自由度的机械臂上基本不变<br>  因为加入的随机抖动对高自由度的来说没有什么帮助，而低自由度的解空间相对比较小，加入后有利于涨点，因为扩大了解空间<br>2.跨域泛化可以通过很好的预训练做到，在新场景中可以只提供干净的场景就可以泛化</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation&lt;/p&gt;
&lt;h3 id=&quot;这篇文章将Diffusion应用到3D空间。是在diffusion-policy基础上进一步提升了策略的能力。&quot;&gt;&lt;a href=&quot;#这篇文章将Diffusion应用到3D空间。是在diffusion-policy基础上进一步提升了策略的能力。&quot; class=&quot;headerlink&quot; title=&quot;这篇文章将Diffusion应用到3D空间。是在diffusion policy基础上进一步提升了策略的能力。&quot;&gt;&lt;/a&gt;这篇文章将Diffusion应用到3D空间。是在diffusion policy基础上进一步提升了策略的能力。&lt;/h3&gt;</summary>
    
    
    
    <category term="readings" scheme="https://fanchenlex.github.io/categories/readings/"/>
    
    
    <category term="paper reading" scheme="https://fanchenlex.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>RoboTwin1.0 | CVPR 2025 | Paper Reading</title>
    <link href="https://fanchenlex.github.io/reandings/RoboTwin1.0/"/>
    <id>https://fanchenlex.github.io/reandings/RoboTwin1.0/</id>
    <published>2025-11-02T03:26:20.000Z</published>
    <updated>2025-11-02T03:40:46.178Z</updated>
    
    <content type="html"><![CDATA[<p>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</p><h3 id="这篇文章主要是为了解决仿真数据生成问题，希望能生成大规模高质量的仿真数据并推动现实世界的机器人操作发展。"><a href="#这篇文章主要是为了解决仿真数据生成问题，希望能生成大规模高质量的仿真数据并推动现实世界的机器人操作发展。" class="headerlink" title="这篇文章主要是为了解决仿真数据生成问题，希望能生成大规模高质量的仿真数据并推动现实世界的机器人操作发展。"></a>这篇文章主要是为了解决仿真数据生成问题，希望能生成大规模高质量的仿真数据并推动现实世界的机器人操作发展。</h3><span id="more"></span><table style="width:100%; border-collapse:collapse;">  <thead style="border-bottom:1px solid #ccc;">    <tr>      <th style="padding:8px; text-align:center;">工作类型(首次/改进)</th>      <th style="padding:8px; text-align:center;">技术路线</th>      <th style="padding:8px; text-align:center;">创新点</th>      <th style="padding:8px; text-align:center;">日期</th>    </tr>  </thead>  <tbody>    <tr>      <td style="padding:8px; text-align:center;">首次</td>      <td style="padding:8px; text-align:center;">simulation data collection</td>      <td style="padding:8px; text-align:center;">自动化仿真数据生成pipline</td>      <td style="padding:8px; text-align:center;">2025-10-03</td>    </tr>  </tbody></table><h2 id="1-What？"><a href="#1-What？" class="headerlink" title="1.What？"></a>1.What？</h2><p>一个针对双臂机器人操作的自动化仿真数据集生成</p><h2 id="2-Why？"><a href="#2-Why？" class="headerlink" title="2.Why？"></a>2.Why？</h2><p>目前具身的仿真环境不够好，仿真和现实世界的GAP太大，仿真数据生成不够自动化，无法起到推动现实世界的作用</p><h2 id="3-How？"><a href="#3-How？" class="headerlink" title="3.How？"></a>3.How？</h2><p><img src="/img/robotwin.png" alt="Data collection Pipeline"></p><ol><li>借助Rodin来实现2D图像-&gt;3D仿真模型，生成了很多3D模型</li><li>设计了一套点轴系统用于推测抓取的一些pose</li><li>作者设计了一套Prompt来引导大模型生成任务的执行代码<br>仿真预训练之后迁移到现实世界</li></ol><h2 id="4-Takeaways"><a href="#4-Takeaways" class="headerlink" title="4.Takeaways:"></a>4.Takeaways:</h2><p>1.二维深度学习(2D DP)在少量样本上性能有限，但随着数据量的增加显著提升<br>2.尽管存在扩展限制，DP3仍展现出强大的少样本能力<br>3.相机视野(FOV)对基于图像的输入任务成功率有显著影响，特别是在块处理任务中，较宽的角度可能会影响抓取评估的准确性。<br>4.双臂性能:<br>对称任务的成功率高(例如，两只手臂同时抓取瓶子)<br>对于不对称任务的成功率较低，特别是那些需要主动碰撞避免的臂之间任务。干<br>局限性：<br>目前只是2指夹爪<br>未来工作：<br>增加更多的模态(触觉等)<br>设计更多的双臂操作任务为benchmark<br>设计更高自由度的机器人末端执行器去设计专家数据的采集方案</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins&lt;/p&gt;
&lt;h3 id=&quot;这篇文章主要是为了解决仿真数据生成问题，希望能生成大规模高质量的仿真数据并推动现实世界的机器人操作发展。&quot;&gt;&lt;a href=&quot;#这篇文章主要是为了解决仿真数据生成问题，希望能生成大规模高质量的仿真数据并推动现实世界的机器人操作发展。&quot; class=&quot;headerlink&quot; title=&quot;这篇文章主要是为了解决仿真数据生成问题，希望能生成大规模高质量的仿真数据并推动现实世界的机器人操作发展。&quot;&gt;&lt;/a&gt;这篇文章主要是为了解决仿真数据生成问题，希望能生成大规模高质量的仿真数据并推动现实世界的机器人操作发展。&lt;/h3&gt;</summary>
    
    
    
    <category term="readings" scheme="https://fanchenlex.github.io/categories/readings/"/>
    
    
    <category term="paper reading" scheme="https://fanchenlex.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>3D Diffusion Policy | RSS 2024(oral) | Paper Reading</title>
    <link href="https://fanchenlex.github.io/reandings/DP3/"/>
    <id>https://fanchenlex.github.io/reandings/DP3/</id>
    <published>2025-10-24T05:15:22.000Z</published>
    <updated>2025-11-02T03:39:48.552Z</updated>
    
    <content type="html"><![CDATA[<p>3D Diffusion Policy:  Generalizable Visuomotor Policy Learning via Simple 3D Representations</p><h3 id="这篇文章将Diffusion应用到3D空间。是在diffusion-policy基础上进一步提升了策略的能力。"><a href="#这篇文章将Diffusion应用到3D空间。是在diffusion-policy基础上进一步提升了策略的能力。" class="headerlink" title="这篇文章将Diffusion应用到3D空间。是在diffusion policy基础上进一步提升了策略的能力。"></a>这篇文章将Diffusion应用到3D空间。是在diffusion policy基础上进一步提升了策略的能力。</h3><span id="more"></span><table style="width:100%; border-collapse:collapse;">  <thead style="border-bottom:1px solid #ccc;">    <tr>      <th style="padding:8px; text-align:center;">工作类型(首次/改进)</th>      <th style="padding:8px; text-align:center;">技术路线</th>      <th style="padding:8px; text-align:center;">创新点</th>      <th style="padding:8px; text-align:center;">日期</th>    </tr>  </thead>  <tbody>    <tr>      <td style="padding:8px; text-align:center;">改进</td>      <td style="padding:8px; text-align:center;">Diffusion Policy</td>      <td style="padding:8px; text-align:center;">将Diffusion Policy扩展到3D空间</td>      <td style="padding:8px; text-align:center;">2025-10-02</td>    </tr>  </tbody></table><h2 id="1-What？"><a href="#1-What？" class="headerlink" title="1.What？"></a>1.What？</h2><p>DP3 &#x3D; DP+3D<br>一个模拟基准( 7 个领域的 72 个不同机器人任务，以及 4 个真实世界任务，包括在可变形物体上进行具有挑战性的灵巧操作)<br>优势：<br>更高的准确性，更少的演示数量和训练轮数<br>泛化至空间、视角、实例和外观等多个方面<br>安全 在实际任务中很少发出不稳定的指令</p><h2 id="2-Why？"><a href="#2-Why？" class="headerlink" title="2.Why？"></a>2.Why？</h2><p>以往仅用3D进行控制的方法推理速度太慢，使用预测和规划的方式低维中有成效但不适合高维控制</p><h2 id="3-How？"><a href="#3-How？" class="headerlink" title="3.How？"></a>3.How？</h2><p><img src="/img/DP3.png" alt="Overview of 3D Diffusion Policy (DP3)"><br>Perception:Encoder: MLP x 3 + maxpooling + Linear + LN （Single viewed）<br>Decision：Decoder: 条件去噪扩散模型 CDDPM<br>  裁剪点云有助于大幅提高精度<br>  加入LayerNorm层有助于稳定不同任务的训练<br>  DP3编码器中的投影头通过将特征投影到较低维度来加速推理，而不会影响精度<br>  去除颜色通道确保了鲁棒的外观泛化<br>  在低维控制任务中，DPM-solver++ 作为噪声采样器与DDIM相比具有竞争力，而DPM-solver++难以很好地处理高维控制任务</p><h2 id="4-Takeaways"><a href="#4-Takeaways" class="headerlink" title="4.Takeaways:"></a>4.Takeaways:</h2><p>简单的点云表示优于其他复杂的三维表示，也更适合扩散策略而非其他策略骨干</p><p>局限性与未来工作：<br>用于控制的最佳 3D 表示仍有待发现<br>这项工作没有深入研究具有极长视野的任务，这有待于未来的探索。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;3D Diffusion Policy:  Generalizable Visuomotor Policy Learning via Simple 3D Representations&lt;/p&gt;
&lt;h3 id=&quot;这篇文章将Diffusion应用到3D空间。是在diffusion-policy基础上进一步提升了策略的能力。&quot;&gt;&lt;a href=&quot;#这篇文章将Diffusion应用到3D空间。是在diffusion-policy基础上进一步提升了策略的能力。&quot; class=&quot;headerlink&quot; title=&quot;这篇文章将Diffusion应用到3D空间。是在diffusion policy基础上进一步提升了策略的能力。&quot;&gt;&lt;/a&gt;这篇文章将Diffusion应用到3D空间。是在diffusion policy基础上进一步提升了策略的能力。&lt;/h3&gt;</summary>
    
    
    
    <category term="readings" scheme="https://fanchenlex.github.io/categories/readings/"/>
    
    
    <category term="paper reading" scheme="https://fanchenlex.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Policy | RSS 2023 | Paper Reading</title>
    <link href="https://fanchenlex.github.io/reandings/DP/"/>
    <id>https://fanchenlex.github.io/reandings/DP/</id>
    <published>2025-10-19T03:05:20.000Z</published>
    <updated>2025-11-02T03:39:56.810Z</updated>
    
    <content type="html"><![CDATA[<p>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</p><h3 id="这篇文章是将Diffusion应用到Robotics的开山之作，引出了很多衍生文章。很值得学习和阅读去了解diffusion-policy。"><a href="#这篇文章是将Diffusion应用到Robotics的开山之作，引出了很多衍生文章。很值得学习和阅读去了解diffusion-policy。" class="headerlink" title="这篇文章是将Diffusion应用到Robotics的开山之作，引出了很多衍生文章。很值得学习和阅读去了解diffusion policy。"></a>这篇文章是将Diffusion应用到Robotics的开山之作，引出了很多衍生文章。很值得学习和阅读去了解diffusion policy。</h3><span id="more"></span><table style="width:100%; border-collapse:collapse;">  <thead style="border-bottom:1px solid #ccc;">    <tr>      <th style="padding:8px; text-align:center;">工作类型(首次/改进)</th>      <th style="padding:8px; text-align:center;">技术路线</th>      <th style="padding:8px; text-align:center;">创新点</th>      <th style="padding:8px; text-align:center;">日期</th>    </tr>  </thead>  <tbody>    <tr>      <td style="padding:8px; text-align:center;">首次</td>      <td style="padding:8px; text-align:center;">Diffusion Policy</td>      <td style="padding:8px; text-align:center;">将Diffusion用到了robotics中</td>      <td style="padding:8px; text-align:center;">2025-10-02</td>    </tr>  </tbody></table><h2 id="1-What？"><a href="#1-What？" class="headerlink" title="1.What？"></a>1.What？</h2><p>将视觉运动策略表示为条件去噪扩散模型，称为扩散策略<br>在机器人动作空间上进行条件去噪扩散过程，根据视觉观测推断动作-评分梯度<br>策略内包含：后退水平控制、视觉条件约束以及时间序列扩散Transformer三种机制，来具体实现模型</p><h2 id="2-Why？"><a href="#2-Why？" class="headerlink" title="2.Why？"></a>2.Why？</h2><p>之前的工作采用高斯混合模型、量化动作的类别表示、通过转换策略表示形式——从显式转为隐式以更好地捕捉多模态分布<br>作者希望将图像领域的扩散模型用到机器人领域，原来生成图像，现在生成动作</p><h2 id="3-How？"><a href="#3-How？" class="headerlink" title="3.How？"></a>3.How？</h2><p><img src="/img/DPmodel.png" alt="Diffusion Policy Overview"><br>ResNet-18(前端抽取图像特征) + CNN-based&#x2F;Transformer-based 模型训练学习不同程度noise加入action的输入输出为原先的groudtruth action学习到这种能力(MSE更新学习FiLM输出的参数以及其他网络参数)后进行推理，输入一个Obs，模型经过k步迭代最后得到预测的下一个Action机器人执行后获得新的Obs，如此反复直到完成任务</p><h2 id="4-Takeaways"><a href="#4-Takeaways" class="headerlink" title="4.Takeaways:"></a>4.Takeaways:</h2><p>归一化处理对于扩散策略获得最佳性能很重要<br>旋转表征：6D旋转表征<br>图像增强：随机裁剪<br>基于CNN的DP优于基于Transformer的DP结构<br>局限性与未来工作：<br>行为克隆的固有局限，如演示数据不足时性能欠佳。扩散策略可应用于强化学习等其他范式，以利用次优和负面数据。<br>相较于LSTM-GMM等简单方法，扩散策略具有更高计算成本与推理延迟。我们的动作序列预测方法部分缓解了该问题，但可能无法满足高频率控制任务需求。<br>未来工作可借助扩散模型加速技术的最新进展来减少所需推理步数。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Diffusion Policy: Visuomotor Policy Learning via Action Diffusion&lt;/p&gt;
&lt;h3 id=&quot;这篇文章是将Diffusion应用到Robotics的开山之作，引出了很多衍生文章。很值得学习和阅读去了解diffusion-policy。&quot;&gt;&lt;a href=&quot;#这篇文章是将Diffusion应用到Robotics的开山之作，引出了很多衍生文章。很值得学习和阅读去了解diffusion-policy。&quot; class=&quot;headerlink&quot; title=&quot;这篇文章是将Diffusion应用到Robotics的开山之作，引出了很多衍生文章。很值得学习和阅读去了解diffusion policy。&quot;&gt;&lt;/a&gt;这篇文章是将Diffusion应用到Robotics的开山之作，引出了很多衍生文章。很值得学习和阅读去了解diffusion policy。&lt;/h3&gt;</summary>
    
    
    
    <category term="readings" scheme="https://fanchenlex.github.io/categories/readings/"/>
    
    
    <category term="paper reading" scheme="https://fanchenlex.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>Southeast University Opening Ceremony</title>
    <link href="https://fanchenlex.github.io/diary/SeuOpeningCeromony/"/>
    <id>https://fanchenlex.github.io/diary/SeuOpeningCeromony/</id>
    <published>2025-09-19T04:12:25.000Z</published>
    <updated>2025-10-01T10:01:02.333Z</updated>
    
    <content type="html"><![CDATA[<h1 id="东大开学典礼"><a href="#东大开学典礼" class="headerlink" title="东大开学典礼"></a>东大开学典礼</h1><p>这是我应该第一次在大学参加户外的开学典礼了，之前本科时的开学典礼受疫情影响改在室内了。</p><span id="more"></span><p><img src="/img/seuarrival.jpg" alt="宿舍门口和父母的合影"><br><img src="/img/seugrassground.jpg" alt="报道当天在大草坪"><br><img src="/img/nameborad.jpg" alt="与自己的名字合影"></p><p><img src="/img/SeuLibrary.jpg" alt="图书馆门口的钢琴"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;东大开学典礼&quot;&gt;&lt;a href=&quot;#东大开学典礼&quot; class=&quot;headerlink&quot; title=&quot;东大开学典礼&quot;&gt;&lt;/a&gt;东大开学典礼&lt;/h1&gt;&lt;p&gt;这是我应该第一次在大学参加户外的开学典礼了，之前本科时的开学典礼受疫情影响改在室内了。&lt;/p&gt;</summary>
    
    
    
    <category term="diary" scheme="https://fanchenlex.github.io/categories/diary/"/>
    
    
    <category term="opening ceremony" scheme="https://fanchenlex.github.io/tags/opening-ceremony/"/>
    
  </entry>
  
  <entry>
    <title>Travel to Nanjing with my parents</title>
    <link href="https://fanchenlex.github.io/diary/nanjing/"/>
    <id>https://fanchenlex.github.io/diary/nanjing/</id>
    <published>2025-09-16T03:05:25.000Z</published>
    <updated>2025-10-01T10:12:52.875Z</updated>
    
    <content type="html"><![CDATA[<h1 id="与父母的南京之行"><a href="#与父母的南京之行" class="headerlink" title="与父母的南京之行"></a>与父母的南京之行</h1><span id="more"></span><p>这次南京之行依旧由我来做攻略，我们9月14日早上飞南京，下飞机后去酒店，酒店正好定在东大四牌楼校区附近(虽然我们最后也没有时间进去)第一天的下午主要就去了总统府，然后看时间似乎来得及就赶着去了一下南京大屠杀遇难同胞纪念馆因为是临时起意所以已经预约不了了，好在母亲有医师证享受了特殊有待就让我们进去了。<br><img src="/img/fuzimiao.jpg" alt="秦淮河夫子庙"><br>晚上去了老门东秦淮河夫子庙，在老门东那边吃东西时还下了一阵暴雨。最后也是体验一波雨中夜游秦淮河了。<br><video src="/video/raining.mp4" controls style="display:block; margin:0 auto; width:100%; height:auto;"></video></p><p style="text-align:center;">雨中的老门东</p><p>第二天去了红山森林动物园，中午去玄武湖转了转，之后去了附近的鸡鸣寺。傍晚去了颐和路拍照打卡了先锋书店。<br><img src="/img/xuanwulake.jpg" alt="玄武湖"><br><img src="/img/yiheRoad.jpg" alt="颐和路"><br>第三天去了南京博物馆，这里怪我没有好好估计时间，南京博物院太大了我们在里面呆了4个多小时才勉强看完。导致后续行程比较赶，即使放弃了明孝陵和美玲宫直接去了中山陵也只是在快要关门前面前逛完之后去了音乐台以及梧桐大道但是由于已经是傍晚所以也没有拍出很清晰的照片。<br><img src="/img/nanjingmuseum.jpg" alt="南京博物馆"><br>以后有时间再在南京转转吧~</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;与父母的南京之行&quot;&gt;&lt;a href=&quot;#与父母的南京之行&quot; class=&quot;headerlink&quot; title=&quot;与父母的南京之行&quot;&gt;&lt;/a&gt;与父母的南京之行&lt;/h1&gt;</summary>
    
    
    
    <category term="diary" scheme="https://fanchenlex.github.io/categories/diary/"/>
    
    
    <category term="travel" scheme="https://fanchenlex.github.io/tags/travel/"/>
    
  </entry>
  
  <entry>
    <title>🏆 National College Student Mathematics Competition (Non-Mathematics A Category)</title>
    <link href="https://fanchenlex.github.io/awards/CMS2023/"/>
    <id>https://fanchenlex.github.io/awards/CMS2023/</id>
    <published>2025-09-07T15:43:59.803Z</published>
    <updated>2025-09-21T02:29:32.236Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>Institution:</strong> Chinese Mathematical Society  </li><li><strong>Date:</strong> December, 2023  </li><li><strong>Honor:</strong> First Prize in Shaanxi Province  </li><li><strong>Ranking:</strong> Top 8%  <span id="more"></span>This award recognizes excellent performance in the highly competitive nationwide mathematics contest for undergraduates.<br><img src="/img/%E5%85%A8%E5%9B%BD%E6%95%B0%E7%AB%9E%E7%9C%81%E4%B8%80.JPG" alt="National College Student Mathematics Competition (Non-Mathematics A Category)"></li></ul>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Institution:&lt;/strong&gt; Chinese Mathematical Society  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; December, 2023  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Honor:&lt;/strong&gt; First Prize in Shaanxi Province  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ranking:&lt;/strong&gt; Top 8%</summary>
    
    
    
    <category term="Awards" scheme="https://fanchenlex.github.io/categories/Awards/"/>
    
    
  </entry>
  
  <entry>
    <title>🏆 Shaanxi Province Advanced Mathematical Competition</title>
    <link href="https://fanchenlex.github.io/awards/SHJK2023/"/>
    <id>https://fanchenlex.github.io/awards/SHJK2023/</id>
    <published>2025-09-07T15:42:10.534Z</published>
    <updated>2025-09-21T02:32:25.460Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>Institution:</strong> Shaanxi Mathematical Society  </li><li><strong>Date:</strong> June, 2023  </li><li><strong>Honor:</strong> Second Prize  <span id="more"></span>This award recognizes my solid mathematical foundation and problem-solving skills in the provincial-level mathematics competition.<br><img src="/img/%E6%95%B0%E7%AB%9E%E7%9C%81%E4%BA%8C.png" alt="Shaanxi Province Advanced Mathematical Competition"></li></ul>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Institution:&lt;/strong&gt; Shaanxi Mathematical Society  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; June, 2023  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Honor:&lt;/strong&gt; Second Prize</summary>
    
    
    
    <category term="Awards" scheme="https://fanchenlex.github.io/categories/Awards/"/>
    
    
  </entry>
  
  <entry>
    <title>🏆 National English Competition for College Students (NECCS)</title>
    <link href="https://fanchenlex.github.io/awards/NECCS2023/"/>
    <id>https://fanchenlex.github.io/awards/NECCS2023/</id>
    <published>2025-09-07T15:42:05.512Z</published>
    <updated>2025-09-21T02:32:22.196Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>Institution:</strong> NECCS Organizing Committee  </li><li><strong>Date:</strong> December, 2022  </li><li><strong>Honor:</strong> National Second Prize  </li><li><strong>Ranking:</strong> Top 3%  <span id="more"></span>The NECCS is a highly influential English proficiency contest in China, testing listening, speaking, reading, and writing abilities of university students.</li></ul><p><img src="/img/%E5%A4%A7%E8%8B%B1%E4%BA%8C%E7%AD%89.png" alt="NECCS"></p><p>Next year!</p><p><img src="/img/%E5%A4%A7%E8%8B%B1%E4%B8%89%E7%AD%89.jpg" alt="NECCS"></p>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Institution:&lt;/strong&gt; NECCS Organizing Committee  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; December, 2022  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Honor:&lt;/strong&gt; National Second Prize  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ranking:&lt;/strong&gt; Top 3%</summary>
    
    
    
    <category term="Awards" scheme="https://fanchenlex.github.io/categories/Awards/"/>
    
    
  </entry>
  
  <entry>
    <title>🏆 National Scholarship</title>
    <link href="https://fanchenlex.github.io/awards/NationalScholarship/"/>
    <id>https://fanchenlex.github.io/awards/NationalScholarship/</id>
    <published>2025-09-07T15:42:03.062Z</published>
    <updated>2025-09-21T03:06:53.230Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>Institution:</strong> Xidian University  </li><li><strong>Date:</strong> May, 2024  </li><li><strong>Honor:</strong> Awarded the National Scholarship  </li><li><strong>Amount:</strong> ¥10,000<span id="more"></span>The National Scholarship is one of the most prestigious honors in Chinese universities, awarded to students with top performance in academics, research, and leadership.<br><img src="/img/NationalScholarship.jpg" alt="National Scholarship Certificate Photo"><br><img src="/img/NationalScholarshipCertificate.jpg" alt="National Scholarship Certificate"><br><img src="/img/NationalScholarshipCertificate_front.jpg" alt="National Scholarship Certificate front page"></li></ul>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Institution:&lt;/strong&gt; Xidian University  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; May, 2024  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Honor:&lt;/strong&gt; Awarded the National Scholarship  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Amount:&lt;/strong&gt; ¥10,000</summary>
    
    
    
    <category term="Awards" scheme="https://fanchenlex.github.io/categories/Awards/"/>
    
    
  </entry>
  
  <entry>
    <title>Huawei &quot;Intelligent Base&quot; Scholarship</title>
    <link href="https://fanchenlex.github.io/awards/HuaweiIntelligentBaseScholarship2023/"/>
    <id>https://fanchenlex.github.io/awards/HuaweiIntelligentBaseScholarship2023/</id>
    <published>2025-09-07T15:42:00.291Z</published>
    <updated>2025-09-21T02:31:24.148Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>Institution:</strong> Xidian University &amp; Huawei  </li><li><strong>Date:</strong> December, 2023  </li><li><strong>Honor:</strong> Huawei “Intelligent Base” Industry-Education Integration Scholarship  </li><li><strong>Amount:</strong> ¥5,000  <span id="more"></span>This scholarship recognizes students who demonstrate strong abilities in innovative research and outstanding potential in the ICT field.<iframe src="/pdf/2023关于公布华为智能基座奖教金奖学金文件.pdf" width="100%" height="600px"></iframe></li></ul>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Institution:&lt;/strong&gt; Xidian University &amp;amp; Huawei  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; December, 2023  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Honor:&lt;/strong&gt; Huawei “Intelligent Base” Industry-Education Integration Scholarship  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Amount:&lt;/strong&gt; ¥5,000</summary>
    
    
    
    <category term="Awards" scheme="https://fanchenlex.github.io/categories/Awards/"/>
    
    
  </entry>
  
  <entry>
    <title>🎓 Excellent Student Award</title>
    <link href="https://fanchenlex.github.io/awards/ExcellentStudentAward/"/>
    <id>https://fanchenlex.github.io/awards/ExcellentStudentAward/</id>
    <published>2025-09-07T15:41:57.494Z</published>
    <updated>2025-09-21T02:31:08.949Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>Institution:</strong> Xidian University  </li><li><strong>Date:</strong> November, 2023  </li><li><strong>Honor:</strong> Excellent Student Award  </li><li><strong>Ranking:</strong> Top 5%  <span id="more"></span></li></ul><h2 id="This-award-recognizes-students-who-achieve-excellence-in-both-academic-and-personal-development-at-Xidian-University"><a href="#This-award-recognizes-students-who-achieve-excellence-in-both-academic-and-personal-development-at-Xidian-University" class="headerlink" title="This award recognizes students who achieve excellence in both academic and personal development at Xidian University."></a>This award recognizes students who achieve excellence in both academic and personal development at Xidian University.</h2><p><img src="/img/%E6%A0%A1%E4%BC%98%E7%A7%80%E5%AD%A6%E7%94%9F.jpg" alt="Outstanding Class Honor"></p><h2 id="Got-it-again-next-year"><a href="#Got-it-again-next-year" class="headerlink" title="Got it again next year!"></a>Got it again next year!</h2><p><img src="/img/%E6%A0%A1%E4%BC%98%E7%A7%80%E5%AD%A6%E7%94%9F2.jpg" alt="Outstanding Class Honor"></p>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Institution:&lt;/strong&gt; Xidian University  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; November, 2023  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Honor:&lt;/strong&gt; Excellent Student Award  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ranking:&lt;/strong&gt; Top 5%</summary>
    
    
    
    <category term="Awards" scheme="https://fanchenlex.github.io/categories/Awards/"/>
    
    
  </entry>
  
  <entry>
    <title>🎓 Excellent Student Cadre Award</title>
    <link href="https://fanchenlex.github.io/awards/ExcellentCadreAward/"/>
    <id>https://fanchenlex.github.io/awards/ExcellentCadreAward/</id>
    <published>2025-09-07T15:41:54.683Z</published>
    <updated>2025-09-21T02:30:46.004Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>Institution:</strong> Xidian University  </li><li><strong>Date:</strong> November, 2023  </li><li><strong>Honor:</strong> Excellent Student Cadre Award  </li><li><strong>Ranking:</strong> Top 2%  <span id="more"></span>This award is granted to student leaders who have made outstanding contributions in class and student union work while maintaining excellent academic performance.<br><img src="/img/%E4%BC%98%E7%A7%80%E7%8F%AD%E5%B9%B2%E9%83%A8.png" alt="Excellent Student Cadre Award"></li></ul>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Institution:&lt;/strong&gt; Xidian University  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date:&lt;/strong&gt; November, 2023  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Honor:&lt;/strong&gt; Excellent Student Cadre Award  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ranking:&lt;/strong&gt; Top 2%</summary>
    
    
    
    <category term="Awards" scheme="https://fanchenlex.github.io/categories/Awards/"/>
    
    
  </entry>
  
  <entry>
    <title>Travel to Chengdu with my uncle and aunt</title>
    <link href="https://fanchenlex.github.io/diary/chengdu/"/>
    <id>https://fanchenlex.github.io/diary/chengdu/</id>
    <published>2025-09-01T03:05:25.000Z</published>
    <updated>2025-10-01T06:20:42.219Z</updated>
    
    <content type="html"><![CDATA[<h1 id="与姨妈姨伯的成都之行"><a href="#与姨妈姨伯的成都之行" class="headerlink" title="与姨妈姨伯的成都之行"></a>与姨妈姨伯的成都之行</h1><span id="more"></span><p>之前一直没去过成都，这次因为姨妈姨伯正好要去成都玩于是我正好帮他们规划这次成都的行程也就有机会赶在开学前去一趟成都。我们一共在成都呆了3天多，第一天去了杜甫草堂、宽窄巷子、人民公园、武侯祠、锦里。第二天在春熙路CityWalk然后去了大慈寺以及成都博物馆最后去步行街溜达了一下。第三天报了一日游的团去了都江堰以及三星堆，最后一天走之前去了东郊记忆拍拍照然后就回来了。整体感觉行程还是ok的就是可能走路有些多姨妈姨伯有点吃不消。我们住在我姐的新家，装修完两年的新房平时没人住。<br><img src="/img/bluelake1.jpg" alt="蓝眼泪"><br><img src="/img/bluelake2.jpg" alt="蓝眼泪"><br><img src="/img/bookstore.jpg" alt="太古里书店"><br>照片放的有些少的原因是姨妈喜欢用抖音拍照然后那个特效拍出来很奇怪所以我实在没法放上来，没眼看哈哈哈。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;与姨妈姨伯的成都之行&quot;&gt;&lt;a href=&quot;#与姨妈姨伯的成都之行&quot; class=&quot;headerlink&quot; title=&quot;与姨妈姨伯的成都之行&quot;&gt;&lt;/a&gt;与姨妈姨伯的成都之行&lt;/h1&gt;</summary>
    
    
    
    <category term="diary" scheme="https://fanchenlex.github.io/categories/diary/"/>
    
    
    <category term="travel" scheme="https://fanchenlex.github.io/tags/travel/"/>
    
  </entry>
  
  <entry>
    <title>Receving postgraduate addmission letter</title>
    <link href="https://fanchenlex.github.io/diary/southeast_letter/"/>
    <id>https://fanchenlex.github.io/diary/southeast_letter/</id>
    <published>2025-07-06T16:00:00.000Z</published>
    <updated>2025-09-21T07:51:18.927Z</updated>
    
    <content type="html"><![CDATA[<p>I received my postgraduate addmission letter on 31st June, 2025!</p><span id="more"></span><p><img src="/img/%E7%A1%95%E5%A3%AB%E5%BD%95%E5%8F%96%E9%80%9A%E7%9F%A5%E4%B9%A6.jpg" alt="Southeast University postgraduate addmission letter front page"><br><img src="/img/%E7%A1%95%E5%A3%AB%E5%BD%95%E5%8F%96%E9%80%9A%E7%9F%A5%E4%B9%A6%E8%83%8C%E9%9D%A2.jpg" alt="Southeast University postgraduate addmission letter back page"><br><img src="/img/master1.jpg" alt="the overall look of the addmission letter"><br><img src="/img/masterfront.jpg" alt="The overall front of the notice"><br><img src="/img/masterpage1.jpg" alt="The fist page front of the notice"><br><img src="/img/masterpage2.jpg" alt="The second page front of the notice"><br><img src="/img/brochure1.jpg" alt="Report Notification page one"><br><img src="/img/brochure2.jpg" alt="Report Notification page two"><br><img src="/img/benefit1.jpg" alt="The letter to all new students(Fisrt page)"><br><img src="/img/benefit2.jpg" alt="The letter to all new students(Second page)"><br><img src="/img/mastercard1.jpg" alt="Student Card(front)"><br><img src="/img/mastercard2.jpg" alt="Student Card(back)"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;I received my postgraduate addmission letter on 31st June, 2025!&lt;/p&gt;</summary>
    
    
    
    <category term="diary" scheme="https://fanchenlex.github.io/categories/diary/"/>
    
    
    <category term="addmission letter" scheme="https://fanchenlex.github.io/tags/addmission-letter/"/>
    
  </entry>
  
  <entry>
    <title>Travel to Kansai, Japan</title>
    <link href="https://fanchenlex.github.io/diary/japan_Kansai/"/>
    <id>https://fanchenlex.github.io/diary/japan_Kansai/</id>
    <published>2025-07-01T06:30:37.000Z</published>
    <updated>2025-09-21T07:51:13.494Z</updated>
    
    <content type="html"><![CDATA[<h1 id="与母亲的日本关西之旅"><a href="#与母亲的日本关西之旅" class="headerlink" title="与母亲的日本关西之旅"></a>与母亲的日本关西之旅</h1><p>大四升研一的这个暑假我和母亲开启了我的第二次出国之旅，很遗憾因为工作的原因父亲没一起来。</p><span id="more"></span><p>我们从咸阳T5航站楼出发，落地大阪关西国际机场，因为事先漏了一个小环节导致过安检就浪费了好一会儿时间，下面是图二是我们落地大阪后来到市区后我拍的第一张照片。<br><img src="/img/JapanT5.jpg" alt="图一：出发机场"><br><img src="/img/japanfirstphoto.jpg" alt="图二：落地第一张照片"><br>第一站难波八坂神社：<br><img src="/img/Nanbobaban.jpg" alt="图三：难波八坂神社"><br>随后到了道顿堀，并配了眼镜~<br><img src="/img/OctopusBalls.jpg" alt="图四：道顿堀章鱼小丸子"><br><img src="/img/glasses.jpg" alt="图五：眼镜超市配了眼镜"><br>第一顿饭选择拉面了，不过又贵又咸QAQ~<br><img src="/img/noodles.jpg" alt="图六：在日本的第一顿"><br>第二天去了USJ<br><img src="/img/USJ.jpg" alt="图七：日本环球影城"><br>去了很火的Mario主题餐厅<br><img src="/img/USJMariorestaurant.jpg" alt="图八：马里奥餐厅"><br>晚上特地去天保山摩天轮看大阪夜景<br><img src="/img/Ferriswheel.jpg" alt="图八：摩天轮"><br>第二天主要参观了大阪世博会<br><img src="/img/YumeshimanakaKonohanaWard.jpg" alt="图九：大阪世博会环形眺望台"><br>第三天早上去了大阪城<br><img src="/img/OsakaCity.jpg" alt="图十：大阪城"><br>然后逛逛商业街之后去了奈良，去了奈良博物馆，到公园喂了小鹿，去了若草山以及一些寺庙，smallcitywalk了一波~<br><img src="/img/NaraMuseum.jpg" alt="图十一：奈良博物馆"><br><img src="/img/seedmountain.jpg" alt="图十二：若草山"><br>在奈良的第二天去了唐招提寺，平城宫之后前往京都<br><img src="/img/%E5%94%90%E6%8B%9B%E6%8F%90%E5%AF%BA.jpg" alt="图十三：唐招提寺"><br>到奈良的第一顿尝了一下sushi Ship<br><img src="/img/sushi.jpg" alt="图十四：寿司船"><br>第二天去了八坂神社，伏见稻荷大社，以及二年坂三年坂，清水寺<br><img src="/img/beautifywater.jpg" alt="图十五：八坂神社美容水"><br><img src="/img/%E7%A8%BB%E8%8D%B7%E5%A4%A7%E7%A4%BE.jpg" alt="图十六：稻荷大社"><br><img src="/img/%E6%B8%85%E6%B0%B4%E5%AF%BA.jpg" alt="图十七：清水寺"><br>第三天去了竹林<br><img src="/img/%E7%AB%B9%E6%9E%97.jpg" alt="图十八：竹林"><br>最后一天去了金阁寺(聪明的一休取景地)<br><img src="/img/%E9%87%91%E9%98%81%E5%AF%BA.jpg" alt="图十九：金阁寺"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;与母亲的日本关西之旅&quot;&gt;&lt;a href=&quot;#与母亲的日本关西之旅&quot; class=&quot;headerlink&quot; title=&quot;与母亲的日本关西之旅&quot;&gt;&lt;/a&gt;与母亲的日本关西之旅&lt;/h1&gt;&lt;p&gt;大四升研一的这个暑假我和母亲开启了我的第二次出国之旅，很遗憾因为工作的原因父亲没一起来。&lt;/p&gt;</summary>
    
    
    
    <category term="diary" scheme="https://fanchenlex.github.io/categories/diary/"/>
    
    
    <category term="travel" scheme="https://fanchenlex.github.io/tags/travel/"/>
    
  </entry>
  
  <entry>
    <title>Memories from Xidian Graduation Ceremony</title>
    <link href="https://fanchenlex.github.io/diary/xidian_memories/"/>
    <id>https://fanchenlex.github.io/diary/xidian_memories/</id>
    <published>2025-06-20T03:05:25.000Z</published>
    <updated>2025-09-22T13:46:48.533Z</updated>
    
    <content type="html"><![CDATA[<h1 id="经过四年的学习我终于毕业了！"><a href="#经过四年的学习我终于毕业了！" class="headerlink" title="经过四年的学习我终于毕业了！"></a>经过四年的学习我终于毕业了！</h1><span id="more"></span><p><img src="/img/graduationpage1.jpg" alt="东门毛主席题字纪念碑"><br><img src="/img/graduationpage2.jpg" alt="礼仪广场"><br><img src="/img/graduationpage3.jpg" alt="图书馆"><br><img src="/img/graduationpage4.jpg" alt="与院长合影"><br><img src="/img/graduationpage5.jpg" alt="和高中同学的一些抽象照(上)"><br><img src="/img/graduationpage6.jpg" alt="和高中同学的一些抽象照(下)"><br><img src="/img/graduationpage7.jpg" alt="与同一个专业班的室友"><br><img src="/img/graduationpage8.jpg" alt="和同一个班的尹懿东合影"><br><img src="/img/mentor.jpg" alt="与导员"><br><img src="/img/%E5%92%8C%E7%88%B6%E6%AF%8D.jpg" alt="和父母"><br><img src="/img/%E4%B8%8E%E5%91%A8%E8%BF%B0%E4%BD%99.jpg" alt="与周述余"><br><img src="/img/%E4%B8%8E%E9%83%AD%E6%9E%AB%E5%88%98%E6%B3%BD%E5%B0%9A.jpg" alt="与郭枫刘泽尚"><br><img src="/img/%E4%B8%8E%E4%BD%95%E6%99%BA%E8%AF%9A.jpg" alt="与何智诚"><br><img src="/img/%E4%BD%93%E8%82%B2%E9%A6%86.jpg" alt="体育馆"><br><img src="/img/%E4%B8%8E%E9%83%9D%E7%81%BF.jpg" alt="与郝灿"><br><img src="/img/%E4%B8%8E%E5%AE%8B%E9%A3%9E%E8%B6%8A.jpg" alt="与宋飞越"><br><img src="/img/%E4%B8%8E%E5%88%98%E5%81%B2%E4%BC%BD.jpg" alt="与刘偲伽"><br><img src="/img/%E4%B8%8E%E9%A2%81%E8%AF%81%E8%80%81%E5%B8%88%E8%BF%9C.jpg" alt="与颁证老师远"><br><img src="/img/%E8%AF%81%E4%B9%A6%E5%B9%B4%E7%BA%A7%E5%90%88%E7%85%A7.jpg" alt="证书年级合照"><br><img src="/img/%E4%B8%8E%E9%A2%81%E5%A5%96%E8%80%81%E5%B8%88%E5%90%88%E5%BD%B1%E8%BF%91.jpg" alt="与颁奖老师合影近"><br><img src="/img/%E6%AF%95%E4%B8%9A%E7%A4%BC%E7%89%A9.jpg" alt="毕业礼物"><br><img src="/img/%E8%AF%81%E4%B9%A6%E7%A4%BC%E7%89%A9.jpg" alt="证书礼物"><br><img src="/img/%E6%AF%95%E4%B8%9A%E7%85%A7.jpg" alt="毕业照"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;经过四年的学习我终于毕业了！&quot;&gt;&lt;a href=&quot;#经过四年的学习我终于毕业了！&quot; class=&quot;headerlink&quot; title=&quot;经过四年的学习我终于毕业了！&quot;&gt;&lt;/a&gt;经过四年的学习我终于毕业了！&lt;/h1&gt;</summary>
    
    
    
    <category term="diary" scheme="https://fanchenlex.github.io/categories/diary/"/>
    
    
    <category term="graduation" scheme="https://fanchenlex.github.io/tags/graduation/"/>
    
  </entry>
  
  <entry>
    <title>new title</title>
    <link href="https://fanchenlex.github.io/courses/index/"/>
    <id>https://fanchenlex.github.io/courses/index/</id>
    <published>2025-06-19T03:05:08.000Z</published>
    <updated>2025-09-10T09:15:02.211Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hello"><a href="#hello" class="headerlink" title="hello"></a>hello</h1><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;hello&quot;&gt;&lt;a href=&quot;#hello&quot; class=&quot;headerlink&quot; title=&quot;hello&quot;&gt;&lt;/a&gt;hello&lt;/h1&gt;</summary>
    
    
    
    <category term="collaboration" scheme="https://fanchenlex.github.io/categories/collaboration/"/>
    
    
  </entry>
  
</feed>
